The following two sections describes the two CUDA implementations made during this project. The first implementation, MiddlePar, was the first iteration on how we could parallelise the mathematical model described earlier. The second implementation, OuterPar, is an improved version of the first that further parallelise. Both CUDA implementations derive directly from the C implementation described earlier.

\subsection{MiddlePar} \hfill \\
In order to parallelise the C implementation and make use of the parallelism of the GPU, we had to analyse the each model of the implementation. Our approach was to locate the most obvious part that could be parallelised, and developed a CUDA off that. Since the solution could not be parallelised entirely, we acknowledged that some calculations would be done on the GPU, the parts that could be parallelised, and the rest would be run on the CPU. The most obvious possibility for a CUDA solution is found within the middle model. Compared to both the outer and the inner model, the steps within the middle model were not very dependent on the previous steps. We say very because they are not completely independent. Middle is calculated by adding the result of k1, k2, k3, k4 and then add it with the y value of the previous step. This means that for each step we can calculate the sum of all the k values before we add it all together. This is a good start since the calculation of either k, is essentially a call to the inner model. This solution would keep the amount of inner model calls per thread to be only 4. The CUDA implementation of this is explained in section :MiddlePar:.

\subsubsection{Implementation} \hfill \\
The implementation of this CUDA solution is apparent in the function OuterDiff, which is the function that needs a result from the middle model. OuterDiff prepares two variables that is to be copied to the GPU. The first is the current x value from outer, that is needed to calculate middle, and an array called kSum. kSum is an array that will be used to store the sum of k1, k2, k3 and k4, calculated by each thread. Having these results, calculating each step in middle can be done much faster, since it is just a matter of adding two double values together for each step. 

Based on the step size it is possible to calculate exactly how many steps middle needs to take. Prior to starting the kernel, we need to make sure that there are enough threads available when parallelising middle, such that there are at least one thread for each step. The amount of threads per block would be set prior to program execution, and before starting the kernel the amount of blocks needed to ensure enough threads would be automatically calculated.

In the following we will explain the kernel implementation; see figure (CODEFIGURE INSERT REFERENCE) for the changes made to middle. 

Once the kernel has been started, each thread define their global thread id by calculating threadIdx.x + blockIdx.x*blockDim.x. This ensures that each thread within the kernel has a unique id, which we will need later on.

We then ensure that only threads, that has an id within the range of middle steps, continue on from line XX. We know that this will cause branch diversion in the last block if the total amount of threads is not equal to the amount of steps, however this is a necessary precaution as we will see soon.

The next couple of lines is used to calculate local variables, that is used to call both Inner(…) and MiddleDiff(…). k1, k2 and k4 is then calculated by calling MiddleDiff(…) and Inner(…). In line YY the weighted average of each k is calculated and stored in the kSum array. This is where it is important that the thread id does not exceed the amount of steps. The size of kSum is exactly the total amount of steps, so if a thread id is larger than this, we would run into problems. A solution could be to always make sure that the number of threads you have is equal to the amount of steps to be taken, however since both thread per block and, as well as middle steps varies it is not feasible, and cannot always be ensured. Once the sum of the k values have been stored in kSum for all threads, the kernel terminates.

\subsubsection{Memory considerations} \hfill \\
This CUDA implementation access certain constant variables quite a lot when calculating inner(…). Since we were ever only going to read these constants, it would be a good idea to place this information in constant memory. Constant memory is considerably faster than global memory(INSERT REFERENCE), with the limitation.

It was also considered to keep kSum in shared memory. Shared memory, like constant memory, is likewise considerably faster than global memory, however it is possible for more than one block to be part of the kernel execution. Shared memory allow tread within the same block to share data, but not between blocks, so shared memory was discarded for this solution, and kSum was kept in global memory.

\subsubsection{NVIDIA Analyser} \hfill \\
TODO

\subsubsection{Implementation shortages} \hfill \\
The solution described above have a few drawbacks. First of all, each time a call to middle are made, a new GPU context would be set up, variables would be copied to the GPU, and a new kernel would be started. Doing this for a low x value, and a high step size, would result create a large amount of kernels. Second, this solution need more than one block to parallelise middle if the amount of threads per block were low. Even though it would speed up the call to middle, it does not speed it up the amount of total calls made to middle. Third, we were not really using a lot of threads or blocks to actually make the calculations, compared to how many threads and blocks are available on modern GPU’s.

\subsection{OuterPar} \hfill \\
To solve these issues the GPU parallelisation of middle would have to be changed to use only one block, and we further had to parallelise the total calls to middle. Further investigation showed that the differential solution of the outer integral, used only the current x value from the outer steps to calculate middle. This effectively meant that we could calculate all x values in advance (we knew the start x value, the end x value as well as the size of each step), and further calculate all calls to middle in advance using these values. These could all be calculated at once on the GPU using only one kernel.

\subsubsection{Implementation} \hfill \\

\subsubsection{Memory considerations} \hfill \\

\subsubsection{NVIDIA Analyser} \hfill \\