%!TEX root = /Users/Nikolaj/Developer/GPU-Project/Report/Report.tex

\subsection{Architecture}
In a traditional computer, CPUs and GPUs coexist as the processing units of the computer and the graphics chip respectively. Historically, the graphics chip has been used to render games and images on screen while the CPU is responsible for a lot of administrative tasks like executing the operating system and the processes running on it. Besides a great deal of floating point operations (flops) this requires that it communicates with other parts of the system through channels that might suffer from high latency which introduces the need for caching. Since caching is a vital function for the CPU it also occupies a lot of chip area, leaving fewer transistors for arithmetic units (ALU) to do actual arithmetic. \\

Conversely, a large number of ALUs is what makes the GPU effective in doing its job because pixel rendering requires a lot of flops per second to create a smooth graphical user experience. The GPU never directly communicates with devices, like the hard drive, but gets its data from on-board memory areas so it does not need a large cache area.\\

Compute Unified Device Architecture (CUDA) is a parallel computing model created by Nvidia in 2006 based on an extension of the C programming language. In a CUDA enabled computing system, the CPU is known as the \emph{host} and GPU(s) are known as \emph{devices}. CUDA programs are executed on the system as whole, meaning that the host and the device cooperate on computing the output. This is advantageous because some computing tasks are only partially suited for parallelisation, meaning that such tasks should be split in smaller parts and executed separately on the most favourable piece of hardware for maximal efficiency.\\

The entry point of a a program is always a procedure executed by the host. When the execution is started, the host is instructed to delegate suitable parts of the tasks to one or several devices embedded as code blocks in \emph{kernels} which is the CUDA term for a function executed on a device. If the task computed by the kernel operates on external data, that data mus be copied (by the host) into device memory before the kernel is invoked. Likewise, when the kernel terminates, the host is responsible for reading the data back and deallocate the memory areas used.\\

The computing resources of the GPU is divided into a number of Streaming Multiprocessors (SMP) each responsible for scheduling a number of threads. Typically, the GPU as a whole executes millions of threads in parallel split between a number of SMPs. Each thread is much more lightweight than normal CPU threads and typically take only a few clock cycles to create and schedule compared to several thousands of cycles on normal CPUs. Thus, thread context switching a very efficient operation. Other than that, CUDA threads are similar to regular CPU threads with registers etc. All threads execute the same kernel simultaneously but work on different pieces of the input data following the "Single Program Multiple Data" (SPMD) paradigm in contrast to CPU threads that are programmed separately. 

Since a kernel is executed by millions of threads and each SMP has an upper limit on the number of threads it can schedule at a time, SMPs load threads in smaller units called blocks. A block is a grouping that can communicate through the shared memory region and will always be executed by the same SMP. Other examples of properties for thread blocks include synchronization, whereby the threads in the block can be synchronized to wait for each other at a specific place in the kernel code. The capacity of an SMP is given both in terms of threads and in terms of blocks, choosing the most limiting factor.

When a kernel is invoked, the user specifies two parameters enclosed in angle brakets before the parameter list. The first parameter selects the number of blocks that should be available for thel execution of the kernel. The second parameter selects the number of threads in each block. The configuration of threads and blocks is normally referred to as the kernel grid because each block is identified by three numbers corresponding to coordinates in a 3-dimensional grid. Likewise each thread is identified within a block using 3 thread coordinates. These identifiers are all available in code through predefined variables and are used to distinquish threads from each other when selecting data for the thread to operate on.

When a number of blocks from the grid are loaded by an SMP, the threads of each block are futher split into units of 32 called warps which is the smallest unit of execution in the sense that two threads from the same warp always execute the same instruction. After some time, one or several blocks are put aside and leave room for new blocks to fill the gap. The user is left without tools to choose which blocks should be loaded, in which order and for how long time the blocks should be executed before new blocks take their place. This is all scheduled by the hardware which gives the CUDA programs a strength because programs can be moved between different machines with different GPUs and automatically utilize the hardware available without changing the source code. The more SMPs available on the chip, the more blocks and threads can execute in parallel.



