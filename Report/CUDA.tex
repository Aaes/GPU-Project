%!TEX root = /Users/Nikolaj/Developer/GPU-Project/Report/Report.tex

\subsection{Architecture}
In a traditional computer, CPUs and GPUs coexist as the processing units of the computer and the graphics chip respectively. Historically the graphics chip have been used to render games and images on screen while the CPU is responsible for a lot of administrative tasks like executing the operating system and the processes running on it. Besides a great deal of floating point operations (flops) this requires that it communicates with other parts of the system through channels that might suffer from high latency which introduces the need for caching. Since caching is a vital function for the CPU it also occupies a lot of chip area, leaving less space for arithmetic units (ALU) to do actual arithmetic. 

Conversely, a large capacity of ALUs is what makes the GPU effective in doing its job because pixel rendering requires a lot of flops per second to create a smooth graphical user experience. The GPU never communicate with high latency devices, like the hard drive, directly but gets its data from on-board memory areas so it doesn't need a large cache area.

Compute Unified Device Architecture (CUDA) is a parallel computing model created by Nvidia in 2006 based on a superset of the C programming language. In a CUDA enabled computing system, the CPU is known as the \emph{host} and GPUs are known as \emph{devices}. Programs are executed on the system as whole, meaning that the host and the device cooperate on computing the output. This is advantageous because some computing tasks are only partially suited for parallelisation, meaning that such tasks should be split in smaller parts and executed separately on the most favourable piece of hardware for maximal performance gain. 

No matter what, a task is always executed on the host. The host can then delegate sub tasks to one or several devices embedded as code blocks in \emph{kernels} which is the CUDA term for a 

Devices can execute kernel functions which is the CUDA term for a function that embeds code to be executed on the device.

Before a kernel execution is started, the host must make sure that all the data needed for the computation is available on the GPU. Conversely, the host is responsible for copying back the result data when the computation is completed. CUDA C exposes functions similar to the well known C functions \emph{malloc} and \emph{free} that allocate and free memory on the device similarly to how it is normally done on the hosting system. Additionally a function exists that performs the actual copying given source and target pointers. When all data is available on the device, the host can trigger the kernel function and start the computation.

\subsubsection{Something}

When a device is assigned a kernel, the computing ressources of the device is partitioned into a number of streaming multiprocessors (SMP) that can each maintain a certain number of threads. The threads of an SMP are  commonly known as a thread block. The total collection of blocks is known as a grid. 

CUDA C extends the language with a whole new API with keywords and functions allowing the programmer to specify where a block of code should be executed, where memory should be allocated and so on. Furthermore, one is able to specify how the hardware ressources on the device should be configured to perform optimally while solving a particular problem, i.e. how many threads to use etc.



As the CUDA compiler uses the host compiler for C to compile traditional C code, any valid C program is accepted by the CUDA compiler, but the code will be executed entirely on the host. To execute kernel functions 


As CUDA C is a superset of traditional C, any valid C program is accepted by the CUDA compiler, but the code is executed entirely on the host. To execute code on the GPU, one defines so called kernel functions which are ordinary C functions tagged with the "global" keyword.