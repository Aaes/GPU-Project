%!TEX root = /Users/Nikolaj/Developer/GPU-Project/Report/Report.tex

\subsection{Architecture}
In a traditional computer, CPUs and GPUs coexist as the processing units of the computer and the graphics chip respectively. Historically, the graphics chip has been used to render games and images on screen while the CPU is responsible for a lot of administrative tasks like executing the operating system and the processes running on it. Besides a great deal of floating point operations (flops) this requires that it communicates with other parts of the system through channels that might suffer from high latency which introduces the need for caching. Since caching is a vital function for the CPU it also occupies a lot of chip area, leaving fewer transistors for arithmetic logical units (ALU) to do actual arithmetic operations. \\

Conversely, a large number of ALUs is what makes the GPU effective in doing its job because pixel rendering requires a lot of flops per second to create a smooth graphical user experience. Where the CPU uses large caches to hide latency from other hardware units, where the GPU makes use of warps and small caches to hide latency from read and write operations. The concept of warps will be discussed later in section \ref{sec:kernelandwarps}.

(INSÆT FIGUR fra CUDA BOG)
%The GPU never directly communicates with devices, like the hard drive, but gets its data from on-board memory areas so it does not need a large cache area.\\

Compute Unified Device Architecture (CUDA) is a parallel computing model created by Nvidia in 2006 based on an extension of the C programming language. In a CUDA enabled computing system, the CPU is known as the \emph{host} and GPU(s) are known as \emph{devices}. CUDA programs are executed on the system as whole, meaning that the host and the device cooperate on computing the output. This is advantageous because some computing tasks are only partially suited for parallelization, meaning that such tasks should be split in smaller parts and executed separately on the most favourable piece of hardware for maximum efficiency.\\

The entry point of a program is always a procedure executed by the host. When the execution is started, the host is instructed to delegate suitable parts of the task to one or several devices embedded as code blocks in \emph{kernels} which is the CUDA term for a function executed on a device. If the task computed by the kernel operates on external data, that data must be copied (by the host) into device memory before the kernel is invoked. Likewise, when the kernel terminates, the host is responsible for reading the data back and deallocate the memory areas used.\\

\subsection{Streaming Multiprocessors}
The computing resources of the GPU is divided into a number of streaming multiprocessors (SMP) each responsible for scheduling a number of threads. Typically, the GPU as a whole executes thousands of threads in parallel split between a number of SMPs. Each thread is much more lightweight than normal CPU threads and typically take only a few clock cycles to create and schedule compared to several thousands of cycles on normal CPUs. Thus, thread context switching is a very efficient operation. Other than that, CUDA threads are similar to regular CPU threads with registers etc. All threads execute the same kernel simultaneously but work on different pieces of the data following the ´´Single Program Multiple Data'' (SPMD) paradigm in contrast to CPU threads that are programmed separately. \\

\subsection{Blocks}
Since a kernel is executed by up to millions of threads and each SMP has an upper limit on the number of threads it can execute at a time, SMPs load threads in groups called blocks. Threads within a block can communicate through the shared memory region and will always be executed by the same SMP once assigned. Other examples of properties for thread blocks include synchronization, whereby the threads in the block can be synchronized to wait for each other at a specific place in the kernel code. The capacity of an SMP is limited both in terms of threads and in terms of blocks. E.g if a block contains 128 threads, the SMP has room for 8 blocks, and if the block contains 256 threads, the SMP has room for only 4 blocks.

\subsection{Kernel Invocation and Warps}
\label{sec:kernelandwarps}
When a kernel is invoked, the user specifies two parameters enclosed in angle brakets before the parameter list. The first parameter selects the number of blocks that should be available for thel execution of the kernel. The second parameter selects the number of threads in each block. The configuration of threads and blocks is normally referred to as the kernel grid because each block is identified by three numbers corresponding to coordinates in a 3-dimensional grid. Likewise each thread is identified within a block using 3 thread coordinates. These identifiers are all available in code through predefined variables and are used to distinquish threads from each other when selecting data for the thread to operate on.

When a number of blocks from the grid are loaded by an SMP, the threads of each block are futher split into units of 32 called warps which is the smallest unit of execution in the sense that two threads from the same warp always execute the same instruction. After some time, one or several blocks are put aside and leave room for new blocks to fill the gap. The user is left without tools to choose which blocks should be loaded, in which order and for how long time the blocks should be executed before new blocks take their place. This is all scheduled by the hardware which gives the CUDA programs a strength because programs can be moved between different machines with different GPUs and automatically utilize the hardware available without changing the source code. The more SMPs available on the chip, the more blocks and threads can execute in parallel.

\subsection{Memory}
A CUDA device exposes different kinds of memory with each their latency and access scope. Since latency has a direct influence on performance, considering which types of memory is used in a CUDA program is important to utilize the full performance potential. Besides latency issues, some threads in an SMP might use more memory than is available in which case the number of threads will be automatically reduced, thus affecting the overall performance of the program. 

The memory model in CUDA is shown in Fig. XX. The figure shows a grid of threads divided into 2 blocks with 2 threads each. Memory types include constant memory, global memory, shared memory and thread registers. Global- and constant memory is the only types of memory that can accessed from the host and is the place where the host can drop data to be used in kernel code using functions similar to the well-known malloc function from traditional C. The kernel threads then access the data directly using their block- and thread IDs. Constant memory is fast to read and write to compared to global memory, but is read-only by the device meaning that results from the kernel is written to global memory where it is subsequently picked up by the host.

Global memory suffers from high latency and has a potential for slowing down the kernel execution if not used with care. This is partly made up for by the constant memory which is slightly faster but only read-only by the device. Shared memory and thread registers on the other hand is extremely fast compared to all other memory types. Shared memory is private to each block, limited in size and offers a mean for threads in the block to communicate by writing intermediate results from their computation where other threads can reach it. Thread registers are private to each thread and is used to store frequently accessed variable that does not need to be accessed by other threads. Keywords and functions exist in CUDA C to allocate/deallocate memory and declare a variable as residing in either of the memory types which directly defines the accessibility of the variable to other threads.